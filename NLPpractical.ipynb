{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9TWeXmdL0cBMwMmZxqv8B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shenoda7/Accordion/blob/main/NLPpractical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kVQav9qsU_Y",
        "outputId": "1094bb34-b90e-4063-dda9-f74eac114696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/fizzbuzz/cleaned-toxic-comments/versions/1\n",
            "Training Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   comment_text   159571 non-null  object \n",
            " 1   id             159571 non-null  object \n",
            " 2   identity_hate  159571 non-null  float64\n",
            " 3   insult         159571 non-null  float64\n",
            " 4   obscene        159571 non-null  float64\n",
            " 5   set            159571 non-null  object \n",
            " 6   severe_toxic   159571 non-null  float64\n",
            " 7   threat         159571 non-null  float64\n",
            " 8   toxic          159571 non-null  float64\n",
            " 9   toxicity       159571 non-null  float64\n",
            "dtypes: float64(7), object(3)\n",
            "memory usage: 12.2+ MB\n",
            "None\n",
            "Testing Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 153164 entries, 0 to 153163\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   comment_text   153149 non-null  object \n",
            " 1   id             153164 non-null  object \n",
            " 2   identity_hate  0 non-null       float64\n",
            " 3   insult         0 non-null       float64\n",
            " 4   obscene        0 non-null       float64\n",
            " 5   set            153164 non-null  object \n",
            " 6   severe_toxic   0 non-null       float64\n",
            " 7   threat         0 non-null       float64\n",
            " 8   toxic          0 non-null       float64\n",
            " 9   toxicity       0 non-null       float64\n",
            "dtypes: float64(7), object(3)\n",
            "memory usage: 11.7+ MB\n",
            "None\n",
            "Accuracy: 0.8241492778982006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.82      0.90    153164\n",
            "         1.0       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.82    153164\n",
            "   macro avg       0.50      0.41      0.45    153164\n",
            "weighted avg       1.00      0.82      0.90    153164\n",
            "\n",
            "Comment: 'You are a wonderful pig!' -> Prediction: Toxic\n",
            "Comment: 'You are so ass.' -> Prediction: Toxic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Download dataset from Kaggle\n",
        "path = kagglehub.dataset_download(\"fizzbuzz/cleaned-toxic-comments\")\n",
        "\n",
        "# Print the path where the dataset is saved\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Load the training and testing datasets\n",
        "train_data = pd.read_csv(f\"{path}/train_preprocessed.csv\")\n",
        "test_data = pd.read_csv(f\"{path}/test_preprocessed.csv\")\n",
        "\n",
        "# Display dataset structure to ensure correct loading\n",
        "print(\"Training Data Info:\")\n",
        "print(train_data.info())\n",
        "print(\"Testing Data Info:\")\n",
        "print(test_data.info())\n",
        "\n",
        "# Step 2: Data preprocessing\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "        return text\n",
        "    return \"\"  # Return empty string if text is not a string\n",
        "\n",
        "# Apply preprocessing to the 'comment_text' column (which contains the text of the comments)\n",
        "train_data['cleaned_comment'] = train_data['comment_text'].apply(preprocess_text)\n",
        "test_data['cleaned_comment'] = test_data['comment_text'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Define features and target for training and testing sets\n",
        "X_train = train_data['cleaned_comment']\n",
        "y_train = train_data['toxic']  # Assuming 'toxic' is the target column\n",
        "X_test = test_data['cleaned_comment']\n",
        "y_test = test_data['toxic']\n",
        "\n",
        "# Handle missing values (NaN) in target columns\n",
        "y_train = y_train.fillna(0)  # Fill NaN in training target with 0 (Non-toxic)\n",
        "y_test = y_test.fillna(0)  # Fill NaN in testing target with 0 (Non-toxic)\n",
        "\n",
        "# Step 4: Text vectorization (TF-IDF)\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Step 5: Model training\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Model evaluation\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 7: Test with new comments\n",
        "def predict_toxicity(comment):\n",
        "    cleaned_comment = preprocess_text(comment)\n",
        "    vectorized_comment = tfidf.transform([cleaned_comment])\n",
        "    prediction = model.predict(vectorized_comment)\n",
        "    return \"Toxic\" if prediction[0] == 1 else \"Non-Toxic\"\n",
        "\n",
        "# Test examples\n",
        "example_comment_1 = \"You are a wonderful pig!\"\n",
        "print(f\"Comment: '{example_comment_1}' -> Prediction: {predict_toxicity(example_comment_1)}\")\n",
        "\n",
        "example_comment_2 = \"You are so ass.\"\n",
        "print(f\"Comment: '{example_comment_2}' -> Prediction: {predict_toxicity(example_comment_2)}\")\n"
      ]
    }
  ]
}